#!/usr/bin/env python3
"""
OpenAI Whisper APIë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ íŒŒì¼ STT
- whisper-1: ê¸°ë³¸ STT ëª¨ë¸
- gpt-4o-transcribe-diarize: í™”ìë¶„ë¦¬(Speaker Diarization) ì§€ì› ëª¨ë¸
"""
import os
import sys
import argparse
import subprocess
import tempfile
import shutil
import json
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™”ìë¶„ë¦¬ ëª¨ë¸ ìƒìˆ˜
DIARIZE_MODEL = "gpt-4o-transcribe-diarize"
DEFAULT_MODEL = "whisper-1"

def split_audio_file(audio_path: Path, chunk_size_mb: int = 20) -> list:
    """
    í° ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        chunk_size_mb: ê° ì²­í¬ì˜ ëª©í‘œ í¬ê¸° (MB)

    Returns:
        ë¶„í• ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    # íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    result = subprocess.run(
        ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
         '-of', 'default=noprint_wrappers=1:nokey=1', str(audio_path)],
        capture_output=True, text=True
    )

    try:
        total_duration = float(result.stdout.strip())
    except ValueError:
        print(f"âŒ íŒŒì¼ ê¸¸ì´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        return []

    file_size_mb = audio_path.stat().st_size / (1024 * 1024)
    num_chunks = int(file_size_mb / chunk_size_mb) + 1
    chunk_duration = total_duration / num_chunks

    print(f"ğŸ“¦ íŒŒì¼ ë¶„í•  ì‹œì‘...")
    print(f"   ì „ì²´ ê¸¸ì´: {total_duration/60:.1f}ë¶„")
    print(f"   ë¶„í•  ê°œìˆ˜: {num_chunks}ê°œ")
    print(f"   ì²­í¬ë‹¹ ê¸¸ì´: {chunk_duration/60:.1f}ë¶„")

    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    temp_dir = Path(tempfile.mkdtemp(prefix="audio_split_"))
    chunk_files = []

    try:
        for i in range(num_chunks):
            start_time = i * chunk_duration
            output_file = temp_dir / f"chunk_{i:03d}.m4a"

            # ffmpegë¡œ ì²­í¬ ì¶”ì¶œ
            cmd = [
                'ffmpeg', '-i', str(audio_path),
                '-ss', str(start_time),
                '-t', str(chunk_duration),
                '-c:a', 'aac',  # WAV ë“± ëª¨ë“  í¬ë§·ì„ m4aë¡œ ì¬ì¸ì½”ë”©
                '-b:a', '128k',  # ì ì • ë¹„íŠ¸ë ˆì´íŠ¸
                '-y',  # ë®ì–´ì“°ê¸°
                str(output_file)
            ]

            subprocess.run(cmd, capture_output=True, check=True)
            chunk_files.append(output_file)
            print(f"   âœ“ ì²­í¬ {i+1}/{num_chunks} ìƒì„±")

        return chunk_files

    except subprocess.CalledProcessError as e:
        print(f"âŒ íŒŒì¼ ë¶„í•  ì‹¤íŒ¨: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        shutil.rmtree(temp_dir, ignore_errors=True)
        return []

def transcribe_with_diarization(audio_path: Path, client=None, language: str = "ko") -> str:
    """
    í™”ìë¶„ë¦¬ë¥¼ í¬í•¨í•œ ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜ (gpt-4o-transcribe-diarize ëª¨ë¸ ì‚¬ìš©)

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        client: OpenAI í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        language: ì–¸ì–´ ì½”ë“œ

    Returns:
        í™”ì ë¼ë²¨ì´ í¬í•¨ëœ ë³€í™˜ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´)
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return ""

    try:
        if client is None:
            client = OpenAI(api_key=api_key)

        with open(audio_path, "rb") as audio:
            # gpt-4o-transcribe-diarize API í˜¸ì¶œ
            # ì°¸ê³ : https://platform.openai.com/docs/models/gpt-4o-transcribe-diarize
            response = client.audio.transcriptions.create(
                model=DIARIZE_MODEL,
                file=audio,
                language=language if language != "auto" else None,
                response_format="diarized_json",
                chunking_strategy="auto"  # 30ì´ˆ ì´ìƒ ì˜¤ë””ì˜¤ì—ì„œ í•„ìˆ˜
            )

        # diarized_json ì‘ë‹µ íŒŒì‹±
        return format_diarized_response(response)

    except Exception as e:
        print(f"âŒ í™”ìë¶„ë¦¬ STT ì‹¤íŒ¨: {e}")
        return ""


def format_diarized_response(response) -> str:
    """
    diarized_json ì‘ë‹µì„ ì½ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        response: OpenAI API ì‘ë‹µ ê°ì²´

    Returns:
        í¬ë§·ëœ í…ìŠ¤íŠ¸ (í™”ì ë¼ë²¨ + íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    """
    try:
        # ì‘ë‹µì„ dictë¡œ ë³€í™˜
        if hasattr(response, 'to_dict'):
            data = response.to_dict()
        elif hasattr(response, 'model_dump'):
            data = response.model_dump()
        else:
            data = response

        # segments ì¶”ì¶œ
        segments = data.get('segments', [])
        if not segments:
            # segmentsê°€ ì—†ìœ¼ë©´ textë§Œ ë°˜í™˜
            return data.get('text', '')

        # í™”ìë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ ì¶œë ¥
        lines = []
        current_speaker = None
        current_texts = []

        for segment in segments:
            speaker = segment.get('speaker', 'Unknown')
            text = segment.get('text', '').strip()
            start = segment.get('start', 0)

            if not text:
                continue

            # í™”ìê°€ ë°”ë€Œë©´ ì´ì „ ë°œì–¸ ì €ì¥
            if speaker != current_speaker:
                if current_speaker and current_texts:
                    lines.append(f"[{current_speaker}] {' '.join(current_texts)}")
                current_speaker = speaker
                current_texts = [text]
            else:
                current_texts.append(text)

        # ë§ˆì§€ë§‰ í™”ì ë°œì–¸ ì¶”ê°€
        if current_speaker and current_texts:
            lines.append(f"[{current_speaker}] {' '.join(current_texts)}")

        return '\n\n'.join(lines)

    except Exception as e:
        print(f"âš ï¸  ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        if hasattr(response, 'text'):
            return response.text
        return str(response)


def transcribe_single_file(audio_path: Path, client=None, language: str = "ko",
                          timestamp: bool = False, model: str = "whisper-1",
                          diarize: bool = False) -> str:
    """
    ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        client: OpenAI í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        language: ì–¸ì–´ ì½”ë“œ
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì—¬ë¶€
        model: Whisper ëª¨ë¸
        diarize: í™”ìë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€ (Trueë©´ gpt-4o-transcribe-diarize ëª¨ë¸ ì‚¬ìš©)

    Returns:
        ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´)
    """
    # í™”ìë¶„ë¦¬ ëª¨ë“œë©´ ì „ìš© í•¨ìˆ˜ ì‚¬ìš©
    if diarize:
        return transcribe_with_diarization(audio_path, client, language)

    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return ""

    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if client is None:
            client = OpenAI(api_key=api_key)

        # ì˜¤ë””ì˜¤ íŒŒì¼ ì—´ê¸°
        with open(audio_path, "rb") as audio:
            # Whisper API í˜¸ì¶œ
            if timestamp:
                response = client.audio.transcriptions.create(
                    model=model,
                    file=audio,
                    language=language if language != "auto" else None,
                    response_format="verbose_json",
                    timestamp_granularities=["segment"]
                )
                # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ í…ìŠ¤íŠ¸ ìƒì„±
                text = ""
                for segment in response.segments:
                    start_time = format_timestamp(segment['start'])
                    text += f"[{start_time}] {segment['text']}\n"
            else:
                response = client.audio.transcriptions.create(
                    model=model,
                    file=audio,
                    language=language if language != "auto" else None,
                    response_format="text"
                )
                text = response

        return text

    except Exception as e:
        print(f"âŒ STT ì‹¤íŒ¨: {e}")
        return ""

def transcribe_audio(audio_path: str, output_path: str = None, language: str = "ko",
                     timestamp: bool = False, model: str = "whisper-1", force: bool = False,
                     diarize: bool = False) -> bool:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        language: ì–¸ì–´ ì½”ë“œ (ko, en, auto ë“±)
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì—¬ë¶€
        model: Whisper ëª¨ë¸ (ê¸°ë³¸: whisper-1)
        force: ê°•ì œ ì‹¤í–‰ (ëŒ€í™”í˜• í™•ì¸ ê±´ë„ˆë›°ê¸°)
        diarize: í™”ìë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€ (gpt-4o-transcribe-diarize ëª¨ë¸ ì‚¬ìš©)

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    audio_file = Path(audio_path)

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not audio_file.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        return False

    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if output_path is None:
        output_path = audio_file.with_suffix('.txt')

    output_file = Path(output_path)

    # íŒŒì¼ í¬ê¸° í™•ì¸ (25MB ì œí•œ)
    file_size_mb = audio_file.stat().st_size / (1024 * 1024)

    # 25MB ì´ˆê³¼ ì‹œ ìë™ ë¶„í• 
    if file_size_mb > 25:
        print(f"âš ï¸  íŒŒì¼ í¬ê¸°ê°€ {file_size_mb:.1f}MBì…ë‹ˆë‹¤. 25MBë¥¼ ì´ˆê³¼í•˜ì—¬ ìë™ ë¶„í• í•©ë‹ˆë‹¤.")
        chunk_files = split_audio_file(audio_file, chunk_size_mb=15)

        if not chunk_files:
            print("âŒ íŒŒì¼ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False

        # ê° ì²­í¬ë¥¼ ë³€í™˜í•˜ê³  ê²°ê³¼ ë³‘í•©
        all_text = ""
        temp_dir = chunk_files[0].parent

        try:
            for i, chunk_file in enumerate(chunk_files):
                print(f"\nğŸ¤ ì²­í¬ {i+1}/{len(chunk_files)} ë³€í™˜ ì¤‘...")
                chunk_text = transcribe_single_file(chunk_file, client=None, language=language,
                                                   timestamp=timestamp, model=model, diarize=diarize)
                if chunk_text:
                    all_text += chunk_text + "\n\n"
                else:
                    print(f"âš ï¸  ì²­í¬ {i+1} ë³€í™˜ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            shutil.rmtree(temp_dir, ignore_errors=True)

            # ê²°ê³¼ ì €ì¥
            if not all_text.strip():
                print("âŒ ëª¨ë“  ì²­í¬ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False

            output_file.write_text(all_text, encoding='utf-8')
            print(f"\nâœ… ë¶„í•  STT ì™„ë£Œ!")
            print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
            print(f"ğŸ“Š í…ìŠ¤íŠ¸ ê¸¸ì´: {len(all_text):,} ê¸€ì")
            return True

        except Exception as e:
            print(f"âŒ ë¶„í•  STT ì‹¤íŒ¨: {e}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            return False

    # 25MB ì´í•˜ íŒŒì¼: ì¼ë°˜ ì²˜ë¦¬
    print(f"ğŸ¤ STT ì‹œì‘: {audio_file.name}")
    print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")
    print(f"ğŸŒ ì–¸ì–´: {language}")
    if diarize:
        print(f"ğŸ‘¥ í™”ìë¶„ë¦¬: í™œì„±í™” (gpt-4o-transcribe-diarize)")

    text = transcribe_single_file(audio_file, client=None, language=language,
                                  timestamp=timestamp, model=model, diarize=diarize)

    if not text:
        print("âŒ STT ì‹¤íŒ¨")
        return False

    # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    output_file.write_text(text, encoding='utf-8')

    print(f"âœ… STT ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
    print(f"ğŸ“Š í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,} ê¸€ì")

    return True


def format_timestamp(seconds: float) -> str:
    """ì´ˆë¥¼ HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"


def main():
    parser = argparse.ArgumentParser(description="OpenAI Whisper APIë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ STT")
    parser.add_argument("audio_file", help="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("-o", "--output", help="ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("-l", "--language", default="ko", help="ì–¸ì–´ ì½”ë“œ (ko, en, auto)")
    parser.add_argument("-t", "--timestamp", action="store_true", help="íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨")
    parser.add_argument("-m", "--model", default="whisper-1", help="Whisper ëª¨ë¸")
    parser.add_argument("-f", "--force", action="store_true", help="ê°•ì œ ì‹¤í–‰ (ëŒ€í™”í˜• í™•ì¸ ê±´ë„ˆë›°ê¸°)")
    parser.add_argument("-d", "--diarize", action="store_true",
                        help="í™”ìë¶„ë¦¬ í™œì„±í™” (gpt-4o-transcribe-diarize ëª¨ë¸ ì‚¬ìš©)")

    args = parser.parse_args()

    success = transcribe_audio(
        args.audio_file,
        args.output,
        args.language,
        args.timestamp,
        args.model,
        args.force,
        args.diarize
    )

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
